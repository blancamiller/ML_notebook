{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7 V2: Neural Net on Phoneme Data for Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objective:__ The aim of neural networks is to extract linear combinations of inputs as derived features to generate a nonlinear model of the data that makes predictions for new data sets [1]. A neural net takes a set of inputs, weights and biases them, and runs them through a series of hidden layers. These hidden layers are composed of nodes that each contain primitive function; nodes add together the weighted inputs they retrieve and applies the primitive function. These primitive functions are called 'activation functions' and they are usually the sigmoid/logistic function (the reLU and hyperbolic rangent functions are two others used) [2]  [3]. After traversing the network of hidden layers, the inputs are transformed into a set of outputs to make predictions about new data [1]. When given a set of data with known labels/targets estimating the optimal neural network weights and biases is computed using back-propogation. For this assignment, a data set of 5 phoneme classifications from continuous data of 50 male speakers were used.\n",
    "\n",
    "the output of a neuron can be the input of another\n",
    "\n",
    "__Forward Propogation:__ calculate \n",
    "\n",
    "__Backpropagation:__ update each existing weight in the network so that they cause the current output value to move closer the target/true output, which is achieved by minimizing the error for each output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables__\n",
    "- x\n",
    "- y\n",
    "- y_hat\n",
    "\n",
    "__Equations__\n",
    "- Sum of Squares Error Function/Loss Function: Error = 1/2 * sum(target_j - output_j)^2\n",
    "- Sigmoid Function: sigmoid σ(v) = 1/(1 + e^(−v))\n",
    "- Weight Update Rule for Single Output Node for Hidden-to-Output Weights:\n",
    "\n",
    "\n",
    "__General Algorithm__\n",
    "\n",
    "_Assumptions_\n",
    "- binary classification \n",
    "- the hidden layer & output layer use the same activation function (this is due to doing binary classification)\n",
    "\n",
    "_Forward Propagation through the Network_\n",
    "- traverse the network forwards from the input layer nodes --> output layer nodes:\n",
    "    - calculate the net input for each hidden layer node and each output layer node\n",
    "    - \"squash\" each net input with the activation function\n",
    "_Backward Propogation through the Network_\n",
    "- traverse the network backwards from the output layer nodes --> input layer nodes:\n",
    "    - calculate the squared error for each output layer node: Error = computed_output(y_hat) - target_output(y)\n",
    "    - calculate the squared error for each hidden layer node: Error = actv_output(o)*(1-actv_output)*sum(weights*delta)\n",
    "    - calculate the difference in weights \n",
    "    \n",
    "    \n",
    "The algorithm terminates when the value of the error function is sufficiently small. This value is usually ... ?\n",
    "\n",
    "__References:__\n",
    "1. Trevor Hastie, Robert Tibshirani, Jerome Friedman, Elements of Statistical Learning: Data mining, inference, and prediction, 2002. Retrieved from: http://web.stanford.edu/~hastie/ElemStatLearn/main.html\n",
    "2. Raul Rojas, Neural Networks: A systematic introduction, 1996. Retrieved from: http://page.mi.fu-berlin.de/rojas/neural/neuron.pdf\n",
    "3. Aurelien Geron, Hands-on machine learning with scikit learn and tensorflow: concepts, tools, and techniques to build intelligent systems, Sebastopol, CA: O'Reilly Media, 2017.\n",
    "\n",
    "\n",
    "- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "- https://brilliant.org/wiki/backpropagation/\n",
    "- https://blogs.msdn.microsoft.com/uk_faculty_connection/2017/07/04/how-to-implement-the-backpropagation-using-python-and-numpy/\n",
    "\n",
    "- http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\n",
    "- http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- https://en.wikipedia.org/wiki/Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Total Net Input:__ net = w1 x i1  +  w2 x i2 + ... + wN x wN + bias1 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Softmax / Logistic Function [3]:__ σ(v) = 1/(1 + e^(−v))\n",
    "\n",
    "This function is used to guarantee a gradient upon taking its derivative. We desire a function that produces a gradient so that when we implement gradient descent and iterate through the parameters, we are guaranteed to make progress and smoothly transition with each step toward convergence. Conversely, if we were to use a function that contains only flat segment, e.g. the step function, we wouldn't know that we were making progress because the gradient would be zero.  \n",
    "\n",
    "More specifically, this equation squashes the total net input, the value that is calculated by summing all of the inputs that go into a node. The term 'squashing' refers to the fact that we are taking values from the number line and bounding them into the range 0 to 1. This is the same range that the ReLU activation function squashes to. As a second example, if we were to be using the hyperbolic tangent function, the sqaushing range would be from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(net_node_input):\n",
    "    \"\"\"net_node_input is the total net input for a given node\"\"\"\n",
    "    return 1.0 / 1.0 + np.exp(-net_node_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
