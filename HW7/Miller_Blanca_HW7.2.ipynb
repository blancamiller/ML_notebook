{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7 V2: Neural Net on Phoneme Data for Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objective:__ The aim of neural networks is to extract linear combinations of inputs as derived features to generate a nonlinear model of the data that makes predictions for new data sets [1]. A neural net takes a set of inputs, weights and biases them, and runs them through a series of hidden layers. These hidden layers are composed of nodes that each contain primitive function; nodes add together the weighted inputs they retrieve and applies the primitive function. These primitive functions are called 'activation functions' and they are usually the sigmoid/logistic function (the reLU and hyperbolic rangent functions are two others used) [2]  [3]. After traversing the network of hidden layers, the inputs are transformed into a set of outputs to make predictions about new data [1]. When given a set of data with known labels/targets estimating the optimal neural network weights and biases is computed using back-propogation. For this assignment, a data set of 5 phoneme classifications from continuous data of 50 male speakers were used.\n",
    "\n",
    "the output of a neuron can be the input of another\n",
    "\n",
    "__Forward Propogation:__ calculate \n",
    "\n",
    "__Backpropagation:__ update each existing weight in the network so that they cause the current output value to move closer the target/true output, which is achieved by minimizing the error for each output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables__\n",
    "- x\n",
    "- y\n",
    "- y_hat\n",
    "\n",
    "__Equations__\n",
    "- Sum of Squares Error Function/Loss Function: Error = 1/2 * sum(target_j - output_j)^2\n",
    "- Sigmoid Function: sigmoid σ(v) = 1/(1 + e^(−v))\n",
    "- Weight Update Rule for Single Output Node for Hidden-to-Output Weights:\n",
    "\n",
    "\n",
    "__General Algorithm__\n",
    "\n",
    "_Assumptions_\n",
    "- binary classification \n",
    "- the hidden layer & output layer use the same activation function (this is due to doing binary classification)\n",
    "\n",
    "_Forward Propagation through the Network_\n",
    "- traverse the network forwards from the input layer nodes --> output layer nodes:\n",
    "    - calculate the net input for each hidden layer node and each output layer node\n",
    "    - \"squash\" each net input with the activation function\n",
    "_Backward Propogation through the Network_\n",
    "- traverse the network backwards from the output layer nodes --> input layer nodes:\n",
    "    - calculate the squared error for each output layer node: Error = computed_output(y_hat) - target_output(y)\n",
    "    - calculate the squared error for each hidden layer node: Error = actv_output(o)*(1-actv_output)*sum(weights*delta)\n",
    "    - calculate the difference in weights \n",
    "    \n",
    "    \n",
    "The algorithm terminates when the value of the error function is sufficiently small. This value is usually ... ?\n",
    "\n",
    "__References:__\n",
    "1. Trevor Hastie, Robert Tibshirani, Jerome Friedman, Elements of Statistical Learning: Data mining, inference, and prediction, 2002. Retrieved from: http://web.stanford.edu/~hastie/ElemStatLearn/main.html\n",
    "2. Raul Rojas, Neural Networks: A systematic introduction, 1996. Retrieved from: http://page.mi.fu-berlin.de/rojas/neural/neuron.pdf\n",
    "3. Aurelien Geron, Hands-on machine learning with scikit learn and tensorflow: concepts, tools, and techniques to build intelligent systems, Sebastopol, CA: O'Reilly Media, 2017.\n",
    "\n",
    "\n",
    "- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "- https://brilliant.org/wiki/backpropagation/\n",
    "- https://blogs.msdn.microsoft.com/uk_faculty_connection/2017/07/04/how-to-implement-the-backpropagation-using-python-and-numpy/\n",
    "\n",
    "- http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\n",
    "- http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- https://en.wikipedia.org/wiki/Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Softmax / Logistic Function [3]:__ σ(v) = 1/(1 + e^(−v))\n",
    "\n",
    "This function is used to guarantee a gradient upon taking the derivative. We desire a function that produces a gradient so that when we implement gradient descent and iterate through the parameters, we are guaranteed to make progress and smoothly transition with each step toward convergence. Conversely, if we were to use a function that contains only flat segment, e.g. the step function, we wouldn't know that we were making progress because the gradient would be zero.  \n",
    "\n",
    "More specifically, this equation squashes the total net input, the value that is calculated by summing all of the inputs that go into a node. The term 'squashing' refers to the fact that we are taking values from the number line and bounding them into the range 0 to 1. This is the same range that the ReLU activation function squashes to. As a second example, if we were to be using the hyperbolic tangent function, the sqaushing range would be from -1 to 1.\n",
    "\n",
    "__Total Net Input:__ net = w1 x i1  +  w2 x i2 + ... + wN x wN + bias1 x 1\n",
    "\n",
    "This function sums all of the inputs for a given node. This summation is composed of products of weights and the values of the input nodes, including bias nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "        \n",
    "    def calc_total_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "        \n",
    "    def calc_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.sigmoid(self.calc_total_net_input())\n",
    "        return self.output\n",
    "        \n",
    "    def sigmoid(self, total_net_input):\n",
    "        return 1.0 / 1.0 + np.exp(-total_net_input)\n",
    "    \n",
    "    def sqr_error(self, target):\n",
    "        return 0.5 * (target - self.output) ** 2\n",
    "        \n",
    "    def error_wrt_output(self):\n",
    "        return self.output * (1 - self.output)\n",
    "    \n",
    "    def pd_total_net_input_wrt_weight(self, index): # WHERE DOES THIS INDEX VALUE COME FROM ????\n",
    "        return self.inputs[index]     \n",
    "    \n",
    "    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def pd_error_wrt_output(self, target):\n",
    "        return -(target - self.output)\n",
    "    \n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def pd_total_net_input_wrt_input(self):\n",
    "        return self.output * (1 - self.output)\n",
    "    \n",
    "    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "    def pd_error_wrt_total_net_input(self, target):\n",
    "        print(\"PD Error Target: {}\".format(target))\n",
    "        print(\"PD Total Net Input: {}\\n\".format(self.pd_total_net_input_wrt_input()))\n",
    "        return self.pd_error_wrt_output(target) * self.pd_total_net_input_wrt_input()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    \n",
    "    def __init__(self, num_neurons, bias):\n",
    "        \n",
    "        # every neuron in a layer shares the same bias\n",
    "        self.bias = bias if bias else random.random()\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "            \n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neuron[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "            \n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calc_output(inputs))\n",
    "        return outputs\n",
    "            \n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "    \n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights=None, hidden_layer_bias=None, output_layer_weights=None, output_layer_bias=None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.hidden_layer1 = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
    "        \n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "        \n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0 \n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if not hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "                \n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0 \n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for h in range(len(self.hidden_layer.neurons)):\n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random())\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "    '''            \n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, next_hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h1 in range(len(self.hidden_layer1.neurons)):\n",
    "            for h0 in range(len(self.hidden_layer.neurons)):\n",
    "                if not hidden_layer1_weights:\n",
    "                    self.hidden_layer1.neurons[h1].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer1.neurons[h1].weights.append(hidden_layer1_weights[weight_num])\n",
    "    '''            \n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "        \n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        # ADD HIDDEN LAYER 2\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "    \n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "        \n",
    "        # 1. Calculate deltas of output neurons\n",
    "        pd_errors_wrt_output_neuron_total_net_input = \n",
    "        [0] * len(self.output_layer.neurons)\n",
    "        print(\"PD ERROR/OUTPUT NEURON TOTAL: {}\".format(pd_errors_wrt_output_neuron_total_net_input))\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            \n",
    "            # ∂E/∂zⱼ\n",
    "            print(\"Index of output layer neuron: {}\".format(o))\n",
    "            print(\"Value of training output: {}\".format(training_outputs[o]))\n",
    "            #print(\"Value of PD Error / PD net input: {}\".format(neurons[o].pd_error_wrt_total_net_input(training_outputs[o])))\n",
    "            print(\"Value of output layer neuron: {}\\n\".format(self.output_layer.neurons[o].pd_error_wrt_total_net_input(training_outputs[o])))\n",
    "            \n",
    "            pd_errors_wrt_output_neuron_total_net_input[o] = \n",
    "            self.output_layer.neurons[o].\n",
    "            pd_error_wrt_total_net_input(training_outputs[o])\n",
    "\n",
    "        \n",
    "        # 2. Calculate deltas of hidden neurons\n",
    "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer0.neurons)):\n",
    "            \n",
    "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "            d_error_wrt_hidden_neuron_output = 0.5        \n",
    "            for o in range(len(self.output_layer.neurons)): # CHANGE TO HIDDEN LAYER 2 NOT OUTPUT ???\n",
    "                                                            # DOES THIS MEAN I NEED ANOTHER FUNCTION FOR HL WRT HL ???\n",
    "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
    "        \n",
    "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
    "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].pd_total_net_input_wrt_input()\n",
    "            \n",
    "        # COMPUTE DELTA FOR HIDDEN LAYER 2 \n",
    "        #for i in range(len(self.hidden_layer1.neurons)):\n",
    "            \n",
    "            #d_error_wrt_hidden_neuron_output = 0\n",
    "            #for p in range(len(self.))\n",
    "        \n",
    "            \n",
    "        # 3. Update weights of output neurons\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
    "                \n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].pd_total_net_input_wrt_weight(w_ho)\n",
    "                \n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "                \n",
    "        # 4. Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "                \n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].pd_total_net_input_wrt_weight(w_ih)\n",
    "                \n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
    "                \n",
    "            # UPDATE HIDDEN LAYER 2\n",
    "                \n",
    "                \n",
    "        \n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            for o in range(len(training_outputs)):\n",
    "                total_error += self.output_layer.neurons[o].sqr_error(training_outputs[o])\n",
    "            return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnn = NeuralNetwork(2, 2, 2, \\n                   hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], \\n                   hidden_layer_bias=0.35, \\n                   output_layer_weights=[0.4, 0.45, 0.5, 0.55], \\n                   output_layer_bias=0.6)\\n\\nfor i in range(10000):\\n    nn.train([0.05, 0.1], [0.01, 0.99])\\n    \\nprint(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nn = NeuralNetwork(2, 2, 2, \n",
    "                   hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], \n",
    "                   hidden_layer_bias=0.35, \n",
    "                   output_layer_weights=[0.4, 0.45, 0.5, 0.55], \n",
    "                   output_layer_bias=0.6)\n",
    "\n",
    "for i in range(10000):\n",
    "    nn.train([0.05, 0.1], [0.01, 0.99])\n",
    "    \n",
    "print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(num_inputs, \n",
    "num_hidden, \n",
    "num_outputs, \n",
    "hidden_layer_weights=None, \n",
    "hidden_layer_bias=None, \n",
    "output_layer_weights=None, \n",
    "output_layer_bias=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load phoneme data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('five_phonemes.txt', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4509, 259)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names       x.1       x.2       x.3       x.4       x.5       x.6  \\\n",
      "0          1   9.85770   9.20711   9.81689   9.01692   9.05675   8.92518   \n",
      "1          2  13.23079  14.19189  15.34428  18.11737  19.53875  18.32726   \n",
      "2          3  10.81889   9.07615   9.77940  12.20135  12.59005  10.53364   \n",
      "3          4  10.53679   9.12147  10.84621  13.92331  13.52476  10.27831   \n",
      "4          5  12.96705  13.69454  14.91182  18.22292  18.45390  17.25760   \n",
      "\n",
      "        x.7       x.8       x.9         ...              x.249     x.250  \\\n",
      "0  11.28308  11.52980  10.79713         ...           12.68076  11.20767   \n",
      "1  17.34169  17.16861  19.63557         ...            8.45714   8.77266   \n",
      "2   8.54693   9.46049  11.96755         ...            5.00824   5.51019   \n",
      "3   8.97459  11.57109  12.35839         ...            5.85688   5.40324   \n",
      "4  17.79614  17.76387  18.99632         ...            8.00151   7.58624   \n",
      "\n",
      "      x.251     x.252     x.253     x.254     x.255    x.256    g  \\\n",
      "0  13.69394  13.72055  12.16628  12.92489  12.51195  9.75527   sh   \n",
      "1   9.59717   8.45336   7.57730   5.38504   9.43063  8.59328   iy   \n",
      "2   5.95725   7.04992   7.02469   6.58416   6.27058  3.85042  dcl   \n",
      "3   6.07126   5.30651   4.27412   3.63384   3.22823  4.63123  dcl   \n",
      "4   6.65202   7.69109   6.93683   7.03600   7.01278  8.52197   aa   \n",
      "\n",
      "               speaker  \n",
      "0  train.dr1.mcpm0.sa1  \n",
      "1  train.dr1.mcpm0.sa1  \n",
      "2  train.dr1.mcpm0.sa1  \n",
      "3  train.dr1.mcpm0.sa1  \n",
      "4  train.dr1.mcpm0.sa1  \n",
      "\n",
      "[5 rows x 259 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data Frame Into Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names       x.1       x.2       x.3       x.4       x.5       x.6  \\\n",
      "0          1   9.85770   9.20711   9.81689   9.01692   9.05675   8.92518   \n",
      "1          2  13.23079  14.19189  15.34428  18.11737  19.53875  18.32726   \n",
      "2          3  10.81889   9.07615   9.77940  12.20135  12.59005  10.53364   \n",
      "3          4  10.53679   9.12147  10.84621  13.92331  13.52476  10.27831   \n",
      "4          5  12.96705  13.69454  14.91182  18.22292  18.45390  17.25760   \n",
      "\n",
      "        x.7       x.8       x.9         ...              x.249     x.250  \\\n",
      "0  11.28308  11.52980  10.79713         ...           12.68076  11.20767   \n",
      "1  17.34169  17.16861  19.63557         ...            8.45714   8.77266   \n",
      "2   8.54693   9.46049  11.96755         ...            5.00824   5.51019   \n",
      "3   8.97459  11.57109  12.35839         ...            5.85688   5.40324   \n",
      "4  17.79614  17.76387  18.99632         ...            8.00151   7.58624   \n",
      "\n",
      "      x.251     x.252     x.253     x.254     x.255    x.256    g  \\\n",
      "0  13.69394  13.72055  12.16628  12.92489  12.51195  9.75527   sh   \n",
      "1   9.59717   8.45336   7.57730   5.38504   9.43063  8.59328   iy   \n",
      "2   5.95725   7.04992   7.02469   6.58416   6.27058  3.85042  dcl   \n",
      "3   6.07126   5.30651   4.27412   3.63384   3.22823  4.63123  dcl   \n",
      "4   6.65202   7.69109   6.93683   7.03600   7.01278  8.52197   aa   \n",
      "\n",
      "               speaker  \n",
      "0  train.dr1.mcpm0.sa1  \n",
      "1  train.dr1.mcpm0.sa1  \n",
      "2  train.dr1.mcpm0.sa1  \n",
      "3  train.dr1.mcpm0.sa1  \n",
      "4  train.dr1.mcpm0.sa1  \n",
      "\n",
      "[5 rows x 259 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data values: get columns 1-(last-1) for all rows\n",
    "X_phonemes = data_set[1:4509, 1:257]\n",
    "\n",
    "# Parse labels: get last column for all rows \n",
    "y_phonemes = data_set[1:4509, 257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (4508, 256)\n",
      "Labels: (4508,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: {}\".format(X_phonemes.shape))\n",
    "print(\"Labels: {}\".format(y_phonemes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.230789999999999 14.191889999999999 15.34428 ... 5.38504 9.43063\n",
      "  8.59328]\n",
      " [10.81889 9.07615 9.7794 ... 6.584160000000001 6.270580000000001\n",
      "  3.8504199999999997]\n",
      " [10.53679 9.12147 10.846210000000001 ... 3.63384 3.22823 4.63123]\n",
      " [12.96705 13.69454 14.91182 ... 7.036 7.01278 8.52197]]\n"
     ]
    }
   ],
   "source": [
    "print(X_phonemes[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iy' 'dcl' 'dcl' 'aa']\n"
     ]
    }
   ],
   "source": [
    "print(y_phonemes[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Test & Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate 2/3 of the data set as training & 1/3 as testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_phonemes, y_phonemes, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme Training Data: (3020, 256)\n",
      "Phoneme Training Labels: (3020,)\n",
      "Phoneme Testing Data: (1488, 256)\n",
      "Phoneme Testing Labels: (1488,)\n"
     ]
    }
   ],
   "source": [
    "# print data & label set dimensionality for verification\n",
    "print(\"Phoneme Training Data: {}\".format(X_train.shape))\n",
    "print(\"Phoneme Training Labels: {}\".format(y_train.shape))\n",
    "print(\"Phoneme Testing Data: {}\".format(X_test.shape))\n",
    "print(\"Phoneme Testing Labels: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.702960000000001 14.042620000000001 16.26874 ... 5.6684\n",
      "  7.952719999999999 7.72456]\n",
      " [8.10171 15.73148 15.967529999999998 ... 3.94521 6.4263900000000005\n",
      "  5.12735]\n",
      " [13.88137 16.639570000000003 17.941570000000002 ... 10.183689999999999\n",
      "  9.45748 6.642580000000001]\n",
      " [12.787 10.38592 10.02926 ... 15.405170000000002 14.671560000000001\n",
      "  14.60944]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.79055 10.34958 11.25291 ... 5.7674900000000004 5.99896\n",
      "  7.174919999999999]\n",
      " [13.51566 9.73408 16.74912 ... 10.370510000000001 10.78328\n",
      "  11.339830000000001]\n",
      " [12.40045 15.39192 18.76651 ... 8.49279 9.160210000000001 9.4957]\n",
      " [12.42842 12.4803 17.085810000000002 ... 7.233639999999999 9.25057\n",
      "  4.22591]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ao' 'ao' 'aa' 'sh']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dcl' 'iy' 'aa' 'iy']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the Phoneme Classifiers from Strings to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_class_to_int_class(y):\n",
    "    for i in range(len(y)):\n",
    "    \n",
    "        if y[i] == 'aa':\n",
    "            y[i] = [0]\n",
    "        elif y[i] == 'ao':\n",
    "            y[i] = [1]\n",
    "        elif y[i] == 'dcl':\n",
    "            y[i] = [2]\n",
    "        elif y[i] == 'iy':\n",
    "            y[i] = [3]\n",
    "        elif y[i] == 'sh':\n",
    "            y[i] = [4]\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int_train = convert_string_class_to_int_class(y_train)\n",
    "y_int_test = convert_string_class_to_int_class(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize Data to Obtain Similar Inputs & Weight Magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blanca/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# set axis to 1 to standardize by sample/vector, rather than by feature \n",
    "X_train = preprocessing.scale(X_train, axis=1)\n",
    "X_test = preprocessing.scale(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After preprocessing the data matrices, add in their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = zip(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3020"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3020"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1]), list([1]), list([0]), ..., list([3]), list([3]),\n",
       "       list([2])], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(len(X_train), 10, len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of training data: 0\n",
      "Value training outputs: [2]\n",
      "Value training inputs: [ 2.13944358  2.20378961  3.75827809  3.54194155  2.93383709  4.15060479\n",
      "  4.28956833  2.99426625  3.54235789  3.62781112  2.51959242  1.22994925\n",
      "  0.24503212  0.27828432  0.65431361  0.82057463 -0.08280065  1.28999495\n",
      "  1.34962431  0.69211265 -0.47025182  0.57990975  0.8220099   1.1865406\n",
      "  1.16878601  0.68540743 -0.79128623  0.72400628  0.19097948 -0.38067356\n",
      " -0.26268575  0.71417853 -0.23900383  0.51720168  0.48853467  0.18762687\n",
      " -0.47324287  0.14436614  0.59682619 -1.38507637  0.36699156  0.3354814\n",
      " -0.65281573 -2.3849159  -0.21261571  0.28711506 -0.50984769 -1.93163961\n",
      " -0.09469913  0.57529717  1.00439303  0.9284279  -0.57920072  0.20226989\n",
      " -0.32548147 -0.89556777 -0.25793622  0.73836992  0.41672744 -0.63752629\n",
      " -0.29682541  0.01125324 -0.12918391 -0.15464622 -0.04288704 -0.60641603\n",
      "  0.34397249  0.29855338 -1.35770219  0.02354067  0.60072661  0.38290551\n",
      "  0.35703233  0.48243752  0.56934245  0.14369233  0.38641698  0.26525735\n",
      " -0.59549813 -2.61581876  0.13595174 -0.33373153  0.34092118 -0.09466078\n",
      " -0.46220445  0.47992306  0.4711745  -0.44315154 -0.52563016 -0.54630459\n",
      " -0.28878353 -0.18832024 -0.9031221   0.02196845 -0.97422813 -0.4425873\n",
      " -0.80838893 -0.9186471   0.33984746 -1.52611063  0.39683637  0.51111549\n",
      "  0.3320521   0.44542732 -0.23058943  0.83537651  0.67622062  0.36986758\n",
      " -0.6086018  -0.10442279  0.14169829 -0.24288234  0.26055164  0.58889387\n",
      "  0.11253825  0.62565755  0.63895295 -1.14673969 -0.5414948  -0.32985301\n",
      "  0.44651199  0.3675832  -0.18546066  0.35350442 -0.14117552  0.16505701\n",
      " -0.92310629 -0.68734433 -0.71194658 -0.0811846  -1.64454216 -0.50031575\n",
      " -0.65784464 -0.03444526 -0.69139814 -0.32407907 -0.55098839 -0.46433544\n",
      "  0.37518683  0.01214617 -1.99136758 -1.04501095  0.29810417  0.29310812\n",
      " -1.89675492 -0.25690634 -1.3479073  -0.42572016  0.17196492 -1.39435631\n",
      " -0.35257627 -0.64247851 -0.84116454 -0.48410599 -1.40928419  0.29538154\n",
      "  0.50549493 -0.18737253  0.21442585  0.46613462  0.01263373  0.16500223\n",
      " -0.23825881 -0.17549596 -0.08554518  0.19958014 -1.23228057 -1.0872418\n",
      " -0.01578677  0.38145928  0.33636886  0.08564614  0.62477009  0.60858772\n",
      "  0.46352704  0.40855408 -0.66372815 -3.82131797 -0.45022928  0.11080168\n",
      " -0.38775129 -1.02986395 -0.2197044   0.35296209 -0.81012002 -0.20904946\n",
      " -0.29210875  0.38378748  0.81057158  0.16162223 -0.57410059 -0.65083813\n",
      "  0.11504723 -0.4468438   0.74503132  0.38474068  0.0402106   0.0273151\n",
      " -0.78592315 -1.08927418  0.09985639  0.27950594 -0.22482097  0.52884269\n",
      " -0.05922281 -0.45844098  0.31968249  0.3077128   0.91356028 -0.44040701\n",
      "  0.2171923  -1.02343811  0.51445166  0.17655011 -0.63859452 -0.2400885\n",
      " -1.41060989 -2.63251608  0.08207441  0.59077834  0.49106556 -0.3674165\n",
      " -1.05608224  0.34647051  0.54843246 -1.19339138 -1.22555344 -0.50215092\n",
      " -0.29330298 -0.56569715 -1.0607496  -0.48804476 -0.19700307 -0.48224891\n",
      " -1.76291343  0.3954778   0.48758147 -0.45779457 -1.53835971 -0.51009968\n",
      "  0.63751221 -0.00955267  0.49828025  0.07021427 -0.12970433 -0.67689208\n",
      " -0.62532103 -0.20823322  0.17433695 -0.07841815 -0.77042554  0.1680645\n",
      "  0.20182617  0.00775272 -0.05040851  0.39026263] \n",
      "\n",
      "PD ERROR WRT OUTPUT NEURON TOTAL: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Index of output layer neuron: 0\n",
      "Value of training output: 2\n",
      "PD Error Target: 2\n",
      "PD Total Net Input: -1.79651848956e-11\n",
      "\n",
      "Value of output layer neuron: 1.79651848953e-11\n",
      "\n",
      "PD Error Target: 2\n",
      "PD Total Net Input: -1.79651848956e-11\n",
      "\n",
      "Index of output layer neuron: 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-364fef796f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value training outputs: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value training inputs: {} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_total_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXy_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3aa63a270106>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_inputs, training_outputs)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# ∂E/∂zⱼ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index of output layer neuron: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value of training output: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;31m#print(\"Value of PD Error / PD net input: {}\".format(neurons[o].pd_error_wrt_total_net_input(training_outputs[o])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value of output layer neuron: {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_error_wrt_total_net_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    training_inputs, training_outputs = random.choice(Xy_train)\n",
    "    print(\"Index of training data: {}\".format(i))\n",
    "    print(\"Value training outputs: {}\".format(training_outputs))\n",
    "    print(\"Value training inputs: {} \\n\".format(training_inputs))\n",
    "    nn.train(training_inputs, training_outputs) # CAN'T GRAB 2nd ELEMENT in t_o\n",
    "    print(i, nn.calculate_total_error(Xy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
