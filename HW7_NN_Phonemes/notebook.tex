
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Miller\_Blanca\_HW7.2}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{HW7 V2: Neural Net on Phoneme Data for Speech
Recognition}\label{hw7-v2-neural-net-on-phoneme-data-for-speech-recognition}

    \textbf{Objective:} The aim of neural networks is to extract linear
combinations of inputs as derived features to generate a nonlinear model
of the data that makes predictions for new data sets {[}1{]}. A neural
net takes a set of inputs, weights and biases them, and runs them
through a series of hidden layers. These hidden layers are composed of
nodes that each contain primitive function; nodes add together the
weighted inputs they retrieve and applies the primitive function. These
primitive functions are called 'activation functions' and they are
usually the sigmoid/logistic function (the reLU and hyperbolic rangent
functions are two others used) {[}2{]} {[}3{]}. After traversing the
network of hidden layers, the inputs are transformed into a set of
outputs to make predictions about new data {[}1{]}. When given a set of
data with known labels/targets estimating the optimal neural network
weights and biases is computed using back-propogation. For this
assignment, a data set of 5 phoneme classifications from continuous data
of 50 male speakers were used.

the output of a neuron can be the input of another

\textbf{Forward Propogation:} calculate

\textbf{Backpropagation:} update each existing weight in the network so
that they cause the current output value to move closer the target/true
output, which is achieved by minimizing the error for each output neuron

    \textbf{Variables} - x - y - y\_hat

\textbf{Equations} - Sum of Squares Error Function/Loss Function: Error
= 1/2 * sum(target\_j - output\_j)\^{}2 - Sigmoid Function: sigmoid σ(v)
= 1/(1 + e\^{}(−v)) - Weight Update Rule for Single Output Node for
Hidden-to-Output Weights:

\textbf{General Algorithm}

\emph{Assumptions} - binary classification - the hidden layer \& output
layer use the same activation function (this is due to doing binary
classification)

\emph{Forward Propagation through the Network} - traverse the network
forwards from the input layer nodes -\/-\textgreater{} output layer
nodes: - calculate the net input for each hidden layer node and each
output layer node - "squash" each net input with the activation function
\emph{Backward Propogation through the Network} - traverse the network
backwards from the output layer nodes -\/-\textgreater{} input layer
nodes: - calculate the squared error for each output layer node: Error =
computed\_output(y\_hat) - target\_output(y) - calculate the squared
error for each hidden layer node: Error =
actv\_output(o)\emph{(1-actv\_output)}sum(weights*delta) - calculate the
difference in weights

The algorithm terminates when the value of the error function is
sufficiently small. This value is usually ... ?

\textbf{References:} 1. Trevor Hastie, Robert Tibshirani, Jerome
Friedman, Elements of Statistical Learning: Data mining, inference, and
prediction, 2002. Retrieved from:
http://web.stanford.edu/\textasciitilde{}hastie/ElemStatLearn/main.html
2. Raul Rojas, Neural Networks: A systematic introduction, 1996.
Retrieved from: http://page.mi.fu-berlin.de/rojas/neural/neuron.pdf 3.
Aurelien Geron, Hands-on machine learning with scikit learn and
tensorflow: concepts, tools, and techniques to build intelligent
systems, Sebastopol, CA: O'Reilly Media, 2017.

\begin{itemize}
\item
  https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/
\item
  https://brilliant.org/wiki/backpropagation/
\item
  https://blogs.msdn.microsoft.com/uk\_faculty\_connection/2017/07/04/how-to-implement-the-backpropagation-using-python-and-numpy/
\item
  http://ufldl.stanford.edu/wiki/index.php/Backpropagation\_Algorithm
\item
  http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/
\item
  https://en.wikipedia.org/wiki/Backpropagation
\end{itemize}

    \textbf{The Softmax / Logistic Function {[}3{]}:} σ(v) = 1/(1 +
e\^{}(−v))

This function is used to guarantee a gradient upon taking the
derivative. We desire a function that produces a gradient so that when
we implement gradient descent and iterate through the parameters, we are
guaranteed to make progress and smoothly transition with each step
toward convergence. Conversely, if we were to use a function that
contains only flat segment, e.g. the step function, we wouldn't know
that we were making progress because the gradient would be zero.

More specifically, this equation squashes the total net input, the value
that is calculated by summing all of the inputs that go into a node. The
term 'squashing' refers to the fact that we are taking values from the
number line and bounding them into the range 0 to 1. This is the same
range that the ReLU activation function squashes to. As a second
example, if we were to be using the hyperbolic tangent function, the
sqaushing range would be from -1 to 1.

\textbf{Total Net Input:} net = w1 x i1 + w2 x i2 + ... + wN x wN +
bias1 x 1

This function sums all of the inputs for a given node. This summation is
composed of products of weights and the values of the input nodes,
including bias nodes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{k+kn}{import} \PY{n+nn}{math}
         \PY{k+kn}{import} \PY{n+nn}{random}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{sklearn}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{preprocessing}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\end{Verbatim}


    \subsubsection{FUNCTIONS}\label{functions}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{class} \PY{n+nc}{Neuron}\PY{p}{:}
            \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{bias}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{bias}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                
            \PY{k}{def} \PY{n+nf}{calc\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{inputs}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{inputs}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                \PY{k}{return} \PY{n}{total} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}
                
            \PY{k}{def} \PY{n+nf}{calc\PYZus{}output}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{inputs} \PY{o}{=} \PY{n}{inputs}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{calc\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}
                
            \PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{total\PYZus{}net\PYZus{}input}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{total\PYZus{}net\PYZus{}input}\PY{p}{)}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{sqr\PYZus{}error}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{target} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
                
            \PY{k}{def} \PY{n+nf}{error\PYZus{}wrt\PYZus{}output}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{pd\PYZus{}total\PYZus{}net\PYZus{}input\PYZus{}wrt\PYZus{}weight}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{index}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} WHERE DOES THIS INDEX VALUE COME FROM ????}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{inputs}\PY{p}{[}\PY{n}{index}\PY{p}{]}     
            
            \PY{c+c1}{\PYZsh{} = ∂E/∂yⱼ = \PYZhy{}(tⱼ \PYZhy{} yⱼ)}
            \PY{k}{def} \PY{n+nf}{pd\PYZus{}error\PYZus{}wrt\PYZus{}output}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{target} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} dyⱼ/dzⱼ = yⱼ * (1 \PYZhy{} yⱼ)}
            \PY{k}{def} \PY{n+nf}{pd\PYZus{}total\PYZus{}net\PYZus{}input\PYZus{}wrt\PYZus{}input}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ}
            \PY{k}{def} \PY{n+nf}{pd\PYZus{}error\PYZus{}wrt\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pd\PYZus{}error\PYZus{}wrt\PYZus{}output}\PY{p}{(}\PY{n}{target}\PY{p}{)} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pd\PYZus{}total\PYZus{}net\PYZus{}input\PYZus{}wrt\PYZus{}input}\PY{p}{(}\PY{p}{)}
            
            
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{class} \PY{n+nc}{NeuronLayer}\PY{p}{:}
            
            \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}neurons}\PY{p}{,} \PY{n}{bias}\PY{p}{)}\PY{p}{:}
                
                \PY{c+c1}{\PYZsh{} every neuron in a layer shares the same bias}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{bias} \PY{k}{if} \PY{n}{bias} \PY{k}{else} \PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}neurons}\PY{p}{)}\PY{p}{:}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{Neuron}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}\PY{p}{)}
                    
            \PY{k}{def} \PY{n+nf}{inspect}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Neurons:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}
                \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Neuron}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n}\PY{p}{)}
                    \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{  Weight:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neuron}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{n}{w}\PY{p}{]}\PY{p}{)}
                    \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{  Bias:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bias}\PY{p}{)}
                    
            \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
                \PY{n}{outputs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{neuron} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons}\PY{p}{:}
                    \PY{n}{outputs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{neuron}\PY{o}{.}\PY{n}{calc\PYZus{}output}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}\PY{p}{)}
                \PY{k}{return} \PY{n}{outputs}
                    
            \PY{k}{def} \PY{n+nf}{get\PYZus{}outputs}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{n}{outputs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                \PY{k}{for} \PY{n}{neuron} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{neurons}\PY{p}{:}
                    \PY{n}{outputs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{neuron}\PY{o}{.}\PY{n}{output}\PY{p}{)}
                \PY{k}{return} \PY{n}{outputs}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{class} \PY{n+nc}{NeuralNetwork}\PY{p}{:}
            \PY{n}{LEARNING\PYZus{}RATE} \PY{o}{=} \PY{l+m+mf}{0.5}
            
            \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{num\PYZus{}inputs}\PY{p}{,} \PY{n}{num\PYZus{}hidden}\PY{p}{,} \PY{n}{num\PYZus{}outputs}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}weights}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}bias}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{,} \PY{n}{output\PYZus{}layer\PYZus{}weights}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{,} \PY{n}{output\PYZus{}layer\PYZus{}bias}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}inputs} \PY{o}{=} \PY{n}{num\PYZus{}inputs}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer} \PY{o}{=} \PY{n}{NeuronLayer}\PY{p}{(}\PY{n}{num\PYZus{}hidden}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}bias}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer1} \PY{o}{=} \PY{n}{NeuronLayer}\PY{p}{(}\PY{n}{num\PYZus{}hidden}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}bias}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer} \PY{o}{=} \PY{n}{NeuronLayer}\PY{p}{(}\PY{n}{num\PYZus{}outputs}\PY{p}{,} \PY{n}{output\PYZus{}layer\PYZus{}bias}\PY{p}{)}
                
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{init\PYZus{}weights\PYZus{}from\PYZus{}inputs\PYZus{}to\PYZus{}hidden\PYZus{}layer\PYZus{}neurons}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}weights}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{init\PYZus{}weights\PYZus{}from\PYZus{}hidden\PYZus{}layer\PYZus{}neurons\PYZus{}to\PYZus{}output\PYZus{}layer\PYZus{}neurons}\PY{p}{(}\PY{n}{output\PYZus{}layer\PYZus{}weights}\PY{p}{)}
                
            \PY{k}{def} \PY{n+nf}{init\PYZus{}weights\PYZus{}from\PYZus{}inputs\PYZus{}to\PYZus{}hidden\PYZus{}layer\PYZus{}neurons}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}weights}\PY{p}{)}\PY{p}{:}
                \PY{n}{weight\PYZus{}num} \PY{o}{=} \PY{l+m+mi}{0} 
                \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}inputs}\PY{p}{)}\PY{p}{:}
                        \PY{k}{if} \PY{o+ow}{not} \PY{n}{hidden\PYZus{}layer\PYZus{}weights}\PY{p}{:}
                            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                        \PY{k}{else}\PY{p}{:}
                            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}weights}\PY{p}{[}\PY{n}{weight\PYZus{}num}\PY{p}{]}\PY{p}{)}
                        \PY{n}{weight\PYZus{}num} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                        
            \PY{k}{def} \PY{n+nf}{init\PYZus{}weights\PYZus{}from\PYZus{}hidden\PYZus{}layer\PYZus{}neurons\PYZus{}to\PYZus{}output\PYZus{}layer\PYZus{}neurons}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{output\PYZus{}layer\PYZus{}weights}\PY{p}{)}\PY{p}{:}
                \PY{n}{weight\PYZus{}num} \PY{o}{=} \PY{l+m+mi}{0} 
                \PY{k}{for} \PY{n}{o} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        \PY{k}{if} \PY{o+ow}{not} \PY{n}{output\PYZus{}layer\PYZus{}weights}\PY{p}{:}
                            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                        \PY{k}{else}\PY{p}{:}
                            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{output\PYZus{}layer\PYZus{}weights}\PY{p}{[}\PY{n}{weight\PYZus{}num}\PY{p}{]}\PY{p}{)}
                        \PY{n}{weight\PYZus{}num} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}            }
        \PY{l+s+sd}{    def init\PYZus{}weights\PYZus{}from\PYZus{}hidden\PYZus{}layer\PYZus{}neurons\PYZus{}to\PYZus{}output\PYZus{}layer\PYZus{}neurons(self, next\PYZus{}hidden\PYZus{}layer\PYZus{}weights):}
        \PY{l+s+sd}{        weight\PYZus{}num = 0}
        \PY{l+s+sd}{        for h1 in range(len(self.hidden\PYZus{}layer1.neurons)):}
        \PY{l+s+sd}{            for h0 in range(len(self.hidden\PYZus{}layer.neurons)):}
        \PY{l+s+sd}{                if not hidden\PYZus{}layer1\PYZus{}weights:}
        \PY{l+s+sd}{                    self.hidden\PYZus{}layer1.neurons[h1].weights.append(random.random())}
        \PY{l+s+sd}{                else:}
        \PY{l+s+sd}{                    self.hidden\PYZus{}layer1.neurons[h1].weights.append(hidden\PYZus{}layer1\PYZus{}weights[weight\PYZus{}num])}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}            
            \PY{k}{def} \PY{n+nf}{inspect}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{* Inputs: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}inputs}\PY{p}{)}\PY{p}{)}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Hidden Layer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{inspect}\PY{p}{(}\PY{p}{)}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{* Output Layer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{inspect}\PY{p}{(}\PY{p}{)}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                
            \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
                \PY{n}{hidden\PYZus{}layer\PYZus{}outputs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} ADD HIDDEN LAYER 2}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}outputs}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{training\PYZus{}inputs}\PY{p}{,} \PY{n}{training\PYZus{}outputs}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{training\PYZus{}inputs}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} 1. Calculate deltas of output neurons}
                \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}output\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}
                \PY{k}{for} \PY{n}{o} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    
                    \PY{c+c1}{\PYZsh{} ∂E/∂zⱼ}
                    \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}output\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{[}\PY{n}{o}\PY{p}{]} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{pd\PYZus{}error\PYZus{}wrt\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{(}\PY{n}{training\PYZus{}outputs}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} 2. Calculate deltas of hidden neurons}
                \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}
                \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    
                    \PY{c+c1}{\PYZsh{} dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ}
                    \PY{n}{d\PYZus{}error\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}output} \PY{o}{=} \PY{l+m+mf}{0.5}        
                    \PY{k}{for} \PY{n}{o} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} CHANGE TO HIDDEN LAYER 2 NOT OUTPUT ???}
                                                                    \PY{c+c1}{\PYZsh{} DOES THIS MEAN I NEED ANOTHER FUNCTION FOR HL WRT HL ???}
                        \PY{n}{d\PYZus{}error\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}output} \PY{o}{+}\PY{o}{=} \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}output\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{[}\PY{n}{o}\PY{p}{]} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{n}{h}\PY{p}{]}
                
                    \PY{c+c1}{\PYZsh{} ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂}
                    \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{[}\PY{n}{h}\PY{p}{]} \PY{o}{=} \PY{n}{d\PYZus{}error\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}output} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{pd\PYZus{}total\PYZus{}net\PYZus{}input\PYZus{}wrt\PYZus{}input}\PY{p}{(}\PY{p}{)}
                    
                \PY{c+c1}{\PYZsh{} COMPUTE DELTA FOR HIDDEN LAYER 2 }
                \PY{c+c1}{\PYZsh{}for i in range(len(self.hidden\PYZus{}layer1.neurons)):}
                    
                    \PY{c+c1}{\PYZsh{}d\PYZus{}error\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}output = 0}
                    \PY{c+c1}{\PYZsh{}for p in range(len(self.))}
                
                    
                \PY{c+c1}{\PYZsh{} 3. Update weights of output neurons}
                \PY{k}{for} \PY{n}{o} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{k}{for} \PY{n}{w\PYZus{}ho} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        
                        \PY{c+c1}{\PYZsh{} ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ}
                        \PY{n}{pd\PYZus{}error\PYZus{}wrt\PYZus{}weight} \PY{o}{=} \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}output\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{[}\PY{n}{o}\PY{p}{]} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{pd\PYZus{}total\PYZus{}net\PYZus{}input\PYZus{}wrt\PYZus{}weight}\PY{p}{(}\PY{n}{w\PYZus{}ho}\PY{p}{)}
                        
                        \PY{c+c1}{\PYZsh{} Δw = α * ∂Eⱼ/∂wᵢ}
                        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{n}{w\PYZus{}ho}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{LEARNING\PYZus{}RATE} \PY{o}{*} \PY{n}{pd\PYZus{}error\PYZus{}wrt\PYZus{}weight}
                        
                \PY{c+c1}{\PYZsh{} 4. Update hidden neuron weights}
                \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{k}{for} \PY{n}{w\PYZus{}ih} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        
                        \PY{c+c1}{\PYZsh{}print(\PYZdq{}==============\PYZdq{})}
                        \PY{c+c1}{\PYZsh{}print(len(self.hidden\PYZus{}layer.neurons))}
                        \PY{c+c1}{\PYZsh{}print(len(self.hidden\PYZus{}layer.neurons[h].weights))}
                        
                        \PY{c+c1}{\PYZsh{} ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ}
                        \PY{n}{pd\PYZus{}error\PYZus{}wrt\PYZus{}weight} \PY{o}{=} \PY{n}{pd\PYZus{}errors\PYZus{}wrt\PYZus{}hidden\PYZus{}neuron\PYZus{}total\PYZus{}net\PYZus{}input}\PY{p}{[}\PY{n}{h}\PY{p}{]} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{pd\PYZus{}total\PYZus{}net\PYZus{}input\PYZus{}wrt\PYZus{}weight}\PY{p}{(}\PY{n}{w\PYZus{}ih}\PY{p}{)}
                        
                        \PY{c+c1}{\PYZsh{} Δw = α * ∂Eⱼ/∂wᵢ}
                        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{hidden\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{weights}\PY{p}{[}\PY{n}{w\PYZus{}ih}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{LEARNING\PYZus{}RATE} \PY{o}{*} \PY{n}{pd\PYZus{}error\PYZus{}wrt\PYZus{}weight}
                        
                    \PY{c+c1}{\PYZsh{} UPDATE HIDDEN LAYER 2}
                        
                        
                
            \PY{k}{def} \PY{n+nf}{calculate\PYZus{}total\PYZus{}error}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{training\PYZus{}sets}\PY{p}{)}\PY{p}{:}
                \PY{n}{total\PYZus{}error} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{training\PYZus{}sets}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{training\PYZus{}inputs}\PY{p}{,} \PY{n}{training\PYZus{}outputs} \PY{o}{=} \PY{n}{training\PYZus{}sets}\PY{p}{[}\PY{n}{t}\PY{p}{]}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{training\PYZus{}inputs}\PY{p}{)}
                    \PY{k}{for} \PY{n}{o} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{training\PYZus{}outputs}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        \PY{n}{total\PYZus{}error} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{output\PYZus{}layer}\PY{o}{.}\PY{n}{neurons}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{o}{.}\PY{n}{sqr\PYZus{}error}\PY{p}{(}\PY{n}{training\PYZus{}outputs}\PY{p}{[}\PY{n}{o}\PY{p}{]}\PY{p}{)}
                    \PY{k}{return} \PY{n}{total\PYZus{}error}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{nn = NeuralNetwork(2, 2, 2, }
        \PY{l+s+sd}{                   hidden\PYZus{}layer\PYZus{}weights=[0.15, 0.2, 0.25, 0.3], }
        \PY{l+s+sd}{                   hidden\PYZus{}layer\PYZus{}bias=0.35, }
        \PY{l+s+sd}{                   output\PYZus{}layer\PYZus{}weights=[0.4, 0.45, 0.5, 0.55], }
        \PY{l+s+sd}{                   output\PYZus{}layer\PYZus{}bias=0.6)}
        
        \PY{l+s+sd}{for i in range(10000):}
        \PY{l+s+sd}{    nn.train([0.05, 0.1], [0.01, 0.99])}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{print(i, round(nn.calculate\PYZus{}total\PYZus{}error([[[0.05, 0.1], [0.01, 0.99]]]), 9))}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} '\textbackslash{}nnn = NeuralNetwork(2, 2, 2, \textbackslash{}n                   hidden\_layer\_weights=[0.15, 0.2, 0.25, 0.3], \textbackslash{}n                   hidden\_layer\_bias=0.35, \textbackslash{}n                   output\_layer\_weights=[0.4, 0.45, 0.5, 0.55], \textbackslash{}n                   output\_layer\_bias=0.6)\textbackslash{}n\textbackslash{}nfor i in range(10000):\textbackslash{}n    nn.train([0.05, 0.1], [0.01, 0.99])\textbackslash{}n    \textbackslash{}nprint(i, round(nn.calculate\_total\_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\textbackslash{}n'
\end{Verbatim}
            
    (num\_inputs, num\_hidden, num\_outputs, hidden\_layer\_weights=None,
hidden\_layer\_bias=None, output\_layer\_weights=None,
output\_layer\_bias=None)

    \paragraph{Load phoneme data set}\label{load-phoneme-data-set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{five\PYZus{}phonemes.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{data}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} (4509, 259)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{print}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
   row.names       x.1       x.2       x.3       x.4       x.5       x.6  \textbackslash{}
0          1   9.85770   9.20711   9.81689   9.01692   9.05675   8.92518   
1          2  13.23079  14.19189  15.34428  18.11737  19.53875  18.32726   
2          3  10.81889   9.07615   9.77940  12.20135  12.59005  10.53364   
3          4  10.53679   9.12147  10.84621  13.92331  13.52476  10.27831   
4          5  12.96705  13.69454  14.91182  18.22292  18.45390  17.25760   

        x.7       x.8       x.9         {\ldots}              x.249     x.250  \textbackslash{}
0  11.28308  11.52980  10.79713         {\ldots}           12.68076  11.20767   
1  17.34169  17.16861  19.63557         {\ldots}            8.45714   8.77266   
2   8.54693   9.46049  11.96755         {\ldots}            5.00824   5.51019   
3   8.97459  11.57109  12.35839         {\ldots}            5.85688   5.40324   
4  17.79614  17.76387  18.99632         {\ldots}            8.00151   7.58624   

      x.251     x.252     x.253     x.254     x.255    x.256    g  \textbackslash{}
0  13.69394  13.72055  12.16628  12.92489  12.51195  9.75527   sh   
1   9.59717   8.45336   7.57730   5.38504   9.43063  8.59328   iy   
2   5.95725   7.04992   7.02469   6.58416   6.27058  3.85042  dcl   
3   6.07126   5.30651   4.27412   3.63384   3.22823  4.63123  dcl   
4   6.65202   7.69109   6.93683   7.03600   7.01278  8.52197   aa   

               speaker  
0  train.dr1.mcpm0.sa1  
1  train.dr1.mcpm0.sa1  
2  train.dr1.mcpm0.sa1  
3  train.dr1.mcpm0.sa1  
4  train.dr1.mcpm0.sa1  

[5 rows x 259 columns]

    \end{Verbatim}

    \paragraph{Convert Data Frame Into Numpy
Array}\label{convert-data-frame-into-numpy-array}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{data\PYZus{}set} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{as\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{print}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
   row.names       x.1       x.2       x.3       x.4       x.5       x.6  \textbackslash{}
0          1   9.85770   9.20711   9.81689   9.01692   9.05675   8.92518   
1          2  13.23079  14.19189  15.34428  18.11737  19.53875  18.32726   
2          3  10.81889   9.07615   9.77940  12.20135  12.59005  10.53364   
3          4  10.53679   9.12147  10.84621  13.92331  13.52476  10.27831   
4          5  12.96705  13.69454  14.91182  18.22292  18.45390  17.25760   

        x.7       x.8       x.9         {\ldots}              x.249     x.250  \textbackslash{}
0  11.28308  11.52980  10.79713         {\ldots}           12.68076  11.20767   
1  17.34169  17.16861  19.63557         {\ldots}            8.45714   8.77266   
2   8.54693   9.46049  11.96755         {\ldots}            5.00824   5.51019   
3   8.97459  11.57109  12.35839         {\ldots}            5.85688   5.40324   
4  17.79614  17.76387  18.99632         {\ldots}            8.00151   7.58624   

      x.251     x.252     x.253     x.254     x.255    x.256    g  \textbackslash{}
0  13.69394  13.72055  12.16628  12.92489  12.51195  9.75527   sh   
1   9.59717   8.45336   7.57730   5.38504   9.43063  8.59328   iy   
2   5.95725   7.04992   7.02469   6.58416   6.27058  3.85042  dcl   
3   6.07126   5.30651   4.27412   3.63384   3.22823  4.63123  dcl   
4   6.65202   7.69109   6.93683   7.03600   7.01278  8.52197   aa   

               speaker  
0  train.dr1.mcpm0.sa1  
1  train.dr1.mcpm0.sa1  
2  train.dr1.mcpm0.sa1  
3  train.dr1.mcpm0.sa1  
4  train.dr1.mcpm0.sa1  

[5 rows x 259 columns]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} Parse data values: get columns 1\PYZhy{}(last\PYZhy{}1) for all rows}
         \PY{n}{X\PYZus{}phonemes} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{4509}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{257}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Parse labels: get last column for all rows }
         \PY{n}{y\PYZus{}phonemes} \PY{o}{=} \PY{n}{data\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{4509}\PY{p}{,} \PY{l+m+mi}{257}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data: \PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{X\PYZus{}phonemes}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Labels: \PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{y\PYZus{}phonemes}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Data: (4508, 256)
Labels: (4508,)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}phonemes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[13.230789999999999 14.191889999999999 15.34428 {\ldots} 5.38504 9.43063
  8.59328]
 [10.81889 9.07615 9.7794 {\ldots} 6.584160000000001 6.270580000000001
  3.8504199999999997]
 [10.53679 9.12147 10.846210000000001 {\ldots} 3.63384 3.22823 4.63123]
 [12.96705 13.69454 14.91182 {\ldots} 7.036 7.01278 8.52197]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k}{print}\PY{p}{(}\PY{n}{y\PYZus{}phonemes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['iy' 'dcl' 'dcl' 'aa']

    \end{Verbatim}

    \paragraph{Generate Test \& Training
Sets}\label{generate-test-training-sets}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} Allocate 2/3 of the data set as training \PYZam{} 1/3 as testing}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}phonemes}\PY{p}{,} \PY{n}{y\PYZus{}phonemes}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} print data \PYZam{} label set dimensionality for verification}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Phoneme Training Data: \PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Phoneme Training Labels: \PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Phoneme Testing Data: \PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Phoneme Testing Labels: \PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Phoneme Training Data: (3020, 256)
Phoneme Training Labels: (3020,)
Phoneme Testing Data: (1488, 256)
Phoneme Testing Labels: (1488,)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[13.2439 13.18735 13.228860000000001 {\ldots} 10.75813 10.742230000000001
  8.61988]
 [8.66446 11.9985 17.52045 {\ldots} 6.06642 7.03607 6.806139999999999]
 [9.142539999999999 9.25641 15.544039999999999 {\ldots} 10.872010000000001
  6.32533 8.65595]
 [13.326889999999999 16.34845 16.299670000000003 {\ldots} 8.06579 7.14575
  8.18154]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[11.55213 13.565129999999998 18.93354 {\ldots} 11.70476 10.42433
  10.824910000000001]
 [9.75758 15.33804 17.65479 {\ldots} 6.35923 6.17282 9.29049]
 [10.700339999999999 9.41404 12.10576 {\ldots} 6.21167 6.24506 3.90079]
 [9.30227 10.856580000000001 14.16259 {\ldots} 6.79949 5.89886 5.53301]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{print}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['aa' 'ao' 'iy' 'ao']

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{print}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['iy' 'aa' 'dcl' 'dcl']

    \end{Verbatim}

    \paragraph{Convert the Phoneme Classifiers from Strings to
Numbers}\label{convert-the-phoneme-classifiers-from-strings-to-numbers}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k}{def} \PY{n+nf}{convert\PYZus{}string\PYZus{}class\PYZus{}to\PYZus{}int\PYZus{}class}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             
                 \PY{k}{if} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{aa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                 \PY{k}{elif} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ao}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                 \PY{k}{elif} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dcl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
                 \PY{k}{elif} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}
                 \PY{k}{elif} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}
                     
             \PY{k}{return} \PY{n}{y}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{y\PYZus{}int\PYZus{}train} \PY{o}{=} \PY{n}{convert\PYZus{}string\PYZus{}class\PYZus{}to\PYZus{}int\PYZus{}class}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}int\PYZus{}test} \PY{o}{=} \PY{n}{convert\PYZus{}string\PYZus{}class\PYZus{}to\PYZus{}int\PYZus{}class}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \paragraph{Standardize Data to Obtain Similar Inputs \& Weight
Magnitudes}\label{standardize-data-to-obtain-similar-inputs-weight-magnitudes}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} set axis to 1 to standardize by sample/vector, rather than by feature }
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{scale}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{scale}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/blanca/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.
  warnings.warn(msg, DataConversionWarning)

    \end{Verbatim}

    \paragraph{After preprocessing the data matrices, add in their
corresponding
labels}\label{after-preprocessing-the-data-matrices-add-in-their-corresponding-labels}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{Xy\PYZus{}train} \PY{o}{=} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:} 3020
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}26}]:} 3020
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{y\PYZus{}train}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} array([list([0]), list([1]), list([3]), {\ldots}, list([2]), list([4]),
                list([4])], dtype=object)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{nn} \PY{o}{=} \PY{n}{NeuralNetwork}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{:}
             \PY{n}{training\PYZus{}inputs}\PY{p}{,} \PY{n}{training\PYZus{}outputs} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{Xy\PYZus{}train}\PY{p}{)}
             \PY{n}{nn}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{training\PYZus{}inputs}\PY{p}{,} \PY{n}{training\PYZus{}outputs}\PY{p}{)} 
             \PY{k}{print}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{nn}\PY{o}{.}\PY{n}{calculate\PYZus{}total\PYZus{}error}\PY{p}{(}\PY{n}{Xy\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(0, 0.2540607486500315)
(1, 0.20453154832290701)
(2, 0.2052494829855624)
(3, 0.20680588041690334)
(4, 0.2018774072811239)
(5, 0.20174507059112184)
(6, 0.2136923686682133)
(7, 0.1985555996927061)
(8, 0.19844079278608692)
(9, 0.19844475429731143)
(10, 0.19844475418365798)
(11, 0.1984146630006758)
(12, 0.19841465895751134)
(13, 0.19841465465727784)
(14, 0.19841408035222288)
(15, 0.19841408041690367)
(16, 0.19841392849429593)
(17, 0.19841432713703008)
(18, 0.1984143286200952)
(19, 0.1984142798050794)
(20, 0.19841427450468963)
(21, 0.1984142815827261)
(22, 0.19841427505436277)
(23, 0.19841423588439858)
(24, 0.19841421680953492)
(25, 0.19841383372448476)
(26, 0.19841374756381508)
(27, 0.198413693500081)
(28, 0.19841367342796198)
(29, 0.19841354026159475)
(30, 0.19841323707429243)
(31, 0.19841324467364393)
(32, 0.19841476420316206)
(33, 0.1984151441571944)
(34, 0.19841514736633115)
(35, 0.19841154314470663)
(36, 0.19841154238045763)
(37, 0.19841154317258047)
(38, 0.19841138251752247)
(39, 0.19841138296949162)
(40, 0.19841281019048665)
(41, 0.19841282320091247)
(42, 0.19841282335624072)
(43, 0.19841283036046015)
(44, 0.19841412233000733)
(45, 0.19841411336545714)
(46, 0.19841411379462726)
(47, 0.19841463205625434)
(48, 0.19841464922127985)
(49, 0.19841464968308348)
(50, 0.19841464441705503)
(51, 0.19841527119127625)
(52, 0.1984152494790212)
(53, 0.19841525198641796)
(54, 0.19841525487594958)
(55, 0.19841542460081876)
(56, 0.19841542486660474)
(57, 0.19841539156506927)
(58, 0.19841539158804675)
(59, 0.19841534751150197)
(60, 0.19841248161005892)
(61, 0.19841248213279902)
(62, 0.1984124111044975)
(63, 0.1984126106820285)
(64, 0.1984126109004297)
(65, 0.19841260995473275)
(66, 0.19841259925593052)
(67, 0.19841259972077321)
(68, 0.1984128716871477)
(69, 0.19841287171267089)
(70, 0.19841247960411512)
(71, 0.1984124798945568)
(72, 0.19855043383578516)
(73, 0.19841760758693253)
(74, 0.1984176016713042)
(75, 0.19841757109562935)
(76, 0.19841779649081132)
(77, 0.19841766327887125)
(78, 0.1984181759342915)
(79, 0.19842064999523776)
(80, 0.19842065584729376)
(81, 0.1984206373580303)
(82, 0.19842066714351367)
(83, 0.19842142125619758)
(84, 0.1984213973538661)
(85, 0.1984214060241386)
(86, 0.19842138738309656)
(87, 0.19842137612067515)
(88, 0.19842137403032575)
(89, 0.19842180580303187)
(90, 0.19842180504877957)
(91, 0.19842180628873746)
(92, 0.19842176345963194)
(93, 0.19842181495250952)
(94, 0.19842193175427916)
(95, 0.19843215229885622)
(96, 0.1984321660121741)
(97, 0.19843216632141636)
(98, 0.19892413144760596)
(99, 0.19892388469130665)
(100, 0.19894660275710063)
(101, 0.19894564247307936)
(102, 0.19898850478210553)
(103, 0.1990075048780546)
(104, 0.29567987607875884)
(105, 0.19841933961529803)
(106, 0.19841822945099621)
(107, 0.19841741818005953)
(108, 0.19841528310257142)
(109, 0.19841625184292164)
(110, 0.19843558080937168)
(111, 0.19850561536785946)
(112, 0.19889607094690293)
(113, 0.19891823861857771)
(114, 0.19892427125853912)
(115, 0.19893229148076969)
(116, 0.19893220594506367)
(117, 0.42151432487357293)
(118, 0.4214704897546482)
(119, 0.4234992780548686)
(120, 0.42612878553752137)
(121, 0.42886349950046376)
(122, 0.19843008566334028)
(123, 0.19842985498599763)
(124, 0.19842341414237366)
(125, 0.1984153996831288)
(126, 0.19841473602224974)
(127, 0.19841350731407556)
(128, 0.1984133519972172)
(129, 0.19841319595257356)
(130, 0.19841313419674028)
(131, 0.19841313383849427)
(132, 0.19841444426119442)
(133, 0.19901118416841784)
(134, 0.19841689111341138)
(135, 0.19841687539517608)
(136, 0.19841802627770472)
(137, 0.19841647362083942)
(138, 0.19841008069439495)
(139, 0.19935776815519723)
(140, 0.19934698496567763)
(141, 0.19935610650009586)
(142, 0.19935603554457057)
(143, 0.1993520332316369)
(144, 0.19935198946544558)
(145, 0.19935986154590296)
(146, 0.19933133707975764)
(147, 0.19886056927801662)
(148, 0.1988598496879139)
(149, 0.19885814763220083)
(150, 0.198680670016941)
(151, 0.1986842461497473)
(152, 0.19868279586619456)
(153, 0.19989725521232976)
(154, 0.1994523595146701)
(155, 0.1993663371815976)
(156, 0.2026353590010284)
(157, 0.2026349745893049)
(158, 0.20263496352467633)
(159, 0.20263450469815078)
(160, 0.19868496814660244)
(161, 0.19868796802447908)
(162, 0.1987424512608091)
(163, 0.1987395727002037)
(164, 0.19904079699593463)
(165, 0.1990403312585339)
(166, 0.1987824810888191)
(167, 0.1987822428365996)
(168, 0.1987819508870773)
(169, 0.19843772231932394)
(170, 0.19843821144796298)
(171, 0.19843821144716514)
(172, 0.19843821096638303)
(173, 0.1984404179804987)
(174, 0.19842339607532103)
(175, 0.1984233826808723)
(176, 0.19840636993391145)
(177, 0.1984063699565023)
(178, 0.19840637072406456)
(179, 0.19840637072405434)
(180, 0.19840637072404743)
(181, 0.1984063811014224)
(182, 0.1999571941547385)
(183, 0.20049538361000432)
(184, 0.19860081636743424)
(185, 0.19859681130889859)
(186, 0.19859607399550827)
(187, 0.19859603545515492)
(188, 0.19858512970677658)
(189, 0.1985853596709121)
(190, 0.19846805876151663)
(191, 0.31129666706667214)
(192, 0.3115723536846079)
(193, 0.20087387153354483)
(194, 0.20088328432279492)
(195, 0.20088327061209799)
(196, 0.19858232054795252)
(197, 0.19858231522837083)
(198, 0.19867153857508635)
(199, 0.1986712348927816)
(200, 0.19867123486880556)
(201, 0.1986699765308363)
(202, 0.19864979661934298)
(203, 0.19864970458758505)
(204, 0.19857412695234186)
(205, 0.19857511978194586)
(206, 0.19857379809895934)
(207, 0.19857290026122193)
(208, 0.4379293859703751)
(209, 0.4378998805793225)
(210, 0.4381072182554245)
(211, 0.198516554433997)
(212, 0.19841269533842262)
(213, 0.19841269320003904)
(214, 0.19840713973504823)
(215, 0.22123785949296398)
(216, 0.22135412442847727)
(217, 0.22134073130017023)
(218, 0.221455174768269)
(219, 0.22145507451524862)
(220, 0.22149600348528894)
(221, 0.2200630697250996)
(222, 0.22005966482242542)
(223, 0.217702762883016)
(224, 0.20216164905204456)
(225, 0.20217597884277208)
(226, 0.20092571083450655)
(227, 0.20098503172914586)
(228, 0.20098501449216416)
(229, 0.20099699051471248)
(230, 0.20099020769972975)
(231, 0.20099789970124432)
(232, 0.20099002160416118)
(233, 0.20099247862576244)
(234, 0.20099172941270907)
(235, 0.2009917227677459)
(236, 0.20099172215201885)
(237, 0.23032171537706284)
(238, 0.2302949945718173)
(239, 0.23042688116632076)
(240, 0.23042106312782606)
(241, 0.20953200880379969)
(242, 0.20692390935257735)
(243, 0.20377547491671297)
(244, 0.2037771580654598)
(245, 0.20379185096551944)
(246, 0.2036192835499496)
(247, 0.20363362198756824)
(248, 0.20364979572771072)
(249, 0.204351826288455)
(250, 0.20437556407647375)
(251, 0.20440576571410962)
(252, 0.2044051217119996)
(253, 0.20435942667942306)
(254, 0.20435926129424914)
(255, 0.20411297377834672)
(256, 0.20412663266632103)
(257, 0.20389890847178435)
(258, 0.2039630293863142)
(259, 0.20397660563855988)
(260, 0.2039687604512863)
(261, 0.19874886187956672)
(262, 0.19874886187955124)
(263, 0.19874967484455963)
(264, 0.19874945646148448)
(265, 0.1987472813283706)
(266, 0.19896882864286067)
(267, 0.19891572041527938)
(268, 0.19891571966119093)
(269, 0.1989174709912134)
(270, 0.19847228134553677)
(271, 0.4826840199001099)
(272, 0.4828105418593193)
(273, 0.19840612779325859)
(274, 0.19840612769041183)
(275, 0.1984061279836481)
(276, 0.19840612798329751)
(277, 0.19840612798329724)
(278, 0.19840648399008823)
(279, 0.1984064848879205)
(280, 0.19840648489272578)
(281, 0.1984064848926783)
(282, 0.1984064848926783)
(283, 0.19840648489267687)
(284, 0.19840647691964536)
(285, 0.1984064780736565)
(286, 0.19840647970005612)
(287, 0.19840647968163921)
(288, 0.19840647968163921)
(289, 0.19840648105739223)
(290, 0.19840648241598197)
(291, 0.1984064824158984)
(292, 0.1984064824158984)
(293, 0.19840648241589756)
(294, 0.198406483311104)
(295, 0.19840647983910564)
(296, 0.19840647983910564)
(297, 0.19952407691697818)
(298, 0.19952404976318436)
(299, 0.19952728999416197)
(300, 0.19952657647748123)
(301, 0.19952975382564314)
(302, 0.19846270747251724)
(303, 0.19841342445660684)
(304, 0.19841342445654453)
(305, 0.19841342443313326)
(306, 0.1984134380221257)
(307, 0.19841343783791515)
(308, 0.19841354691382734)
(309, 0.19841354691211996)
(310, 0.19841352970668194)
(311, 0.19841351518495434)
(312, 0.19841353488957403)
(313, 0.1984135098189993)
(314, 0.19841137439540235)
(315, 0.19841137488345081)
(316, 0.19841138882356105)
(317, 0.1984113927743577)
(318, 0.1984113922996657)
(319, 0.19841140594500134)
(320, 0.19841141359491765)
(321, 0.19841141359484127)
(322, 0.19841142251599045)
(323, 0.19841114968400392)
(324, 0.1984111496836857)
(325, 0.1984111621977254)
(326, 0.19841117457951762)
(327, 0.19841118683175507)
(328, 0.19841119895714301)
(329, 0.19841121095716915)
(330, 0.19841121095693864)
(331, 0.19841122283575283)
(332, 0.1984102698504578)
(333, 0.1984102793303497)
(334, 0.198410376231925)
(335, 0.19841037948802864)
(336, 0.1984103887848035)
(337, 0.19841036244402513)
(338, 0.1984103716659628)
(339, 0.19841037166519013)
(340, 0.19841036270576984)
(341, 0.19841034858478007)
(342, 0.19841024174036503)
(343, 0.19841024173156324)
(344, 0.19841025047017913)
(345, 0.19841449667805297)
(346, 0.19841461929205662)
(347, 0.19841412284643808)
(348, 0.19841401757989535)
(349, 0.19841403362486273)
(350, 0.19841402834744282)
(351, 0.1984140283456288)
(352, 0.1984140283447261)
(353, 0.1984140388676153)
(354, 0.19841404690903294)
(355, 0.19840632687031903)
(356, 0.19840614635283038)
(357, 0.19840614653767774)
(358, 0.1984061465538763)
(359, 0.1984061465537377)
(360, 0.19840614655373673)
(361, 0.19840614655373673)
(362, 0.19872280487718008)
(363, 0.19841956446260808)
(364, 0.19841957288523665)
(365, 0.19841973543388572)
(366, 0.1984197354338583)
(367, 0.1984197354338553)
(368, 0.19842233418144953)
(369, 0.19842220026249244)
(370, 0.19842220026249208)
(371, 0.1984222002603842)
(372, 0.1984224276941695)
(373, 0.19842242769384708)
(374, 0.1984224430850261)
(375, 0.19842190159473674)
(376, 0.1984222258282643)
(377, 0.19842221990123024)
(378, 0.1984163246463282)
(379, 0.19841631425451248)
(380, 0.1984134459023926)
(381, 0.1984134458925772)
(382, 0.1984134463009391)
(383, 0.1984134592903921)
(384, 0.19841347217439606)
(385, 0.221171648693083)
(386, 0.22119676824145273)
(387, 0.21958165644885708)
(388, 0.2195536618374342)
(389, 0.2195479523431848)
(390, 0.19890491437013758)
(391, 0.19890527659171534)
(392, 0.19891631578189994)
(393, 0.19891733684384683)
(394, 0.19891788616250536)
(395, 0.19891748695221204)
(396, 0.19869061591767204)
(397, 0.19868033867276197)
(398, 0.19868062963472558)
(399, 0.19868091908570396)
(400, 0.19868091908565083)
(401, 0.19868091899505744)
(402, 0.19868099135551215)
(403, 0.19868090007224365)
(404, 0.19841500145313273)
(405, 0.19841500145300567)
(406, 0.1984150153779502)
(407, 0.19841501538585862)
(408, 0.19841502272297284)
(409, 0.19841525391993883)
(410, 0.19841525742757893)
(411, 0.1984152714533032)
(412, 0.19841527140990306)
(413, 0.19841527881705617)
(414, 0.19841525702586624)
(415, 0.19841527469667683)
(416, 0.19841527469597792)
(417, 0.1984152745720031)
(418, 0.19841527054728086)
(419, 0.1984152774989448)
(420, 0.19845736342202916)
(421, 0.19845736342098066)
(422, 0.19845741341214002)
(423, 0.19845741341213624)
(424, 0.19845741341189577)
(425, 0.19845746304300352)
(426, 0.19845746304066195)
(427, 0.19845751232980366)
(428, 0.19845751232972608)
(429, 0.19845756162352102)
(430, 0.19845756161643485)
(431, 0.19845756158009345)
(432, 0.198457561569702)
(433, 0.19845763516423462)
(434, 0.19845770825553552)
(435, 0.19845772806227677)
(436, 0.19845780048680287)
(437, 0.1984584878850742)
(438, 0.1984584788730377)
(439, 0.19845836799742841)
(440, 0.19844332183738542)
(441, 0.19844332183730087)
(442, 0.1984433218293513)
(443, 0.1984433727974803)
(444, 0.19844337279740185)
(445, 0.19844337279604776)
(446, 0.1984433727935382)
(447, 0.19844339441970743)
(448, 0.1984434448138742)
(449, 0.19888421551413768)
(450, 0.19888485056166724)
(451, 0.19888527014134055)
(452, 0.19870382467429396)
(453, 0.19870382467661415)
(454, 0.1987038244130684)
(455, 0.19870407215251099)
(456, 0.1987040721525062)
(457, 0.19870446099164746)
(458, 0.1987047122146135)
(459, 0.19870490096549126)
(460, 0.19870466907093176)
(461, 0.19870491703851964)
(462, 0.19868650556424372)
(463, 0.1984094190450672)
(464, 0.1984093787411547)
(465, 0.19840937874096753)
(466, 0.1984093787409481)
(467, 0.19840606129043725)
(468, 0.19840605550192772)
(469, 0.19840605530225747)
(470, 0.19840605003871978)
(471, 0.19840604947866508)
(472, 0.19840604947875146)
(473, 0.19840604947875132)
(474, 0.198406049478749)
(475, 0.1984060494871001)
(476, 0.19840604948739896)
(477, 0.1984060494876968)
(478, 0.19840604894035319)
(479, 0.1984060489313721)
(480, 0.19840604893133582)
(481, 0.19840604892597036)
(482, 0.19840604892597014)
(483, 0.19840604868931297)
(484, 0.19840604868933515)
(485, 0.19840604868934275)
(486, 0.19840604868505587)
(487, 0.19840604868528675)
(488, 0.19840604873331563)
(489, 0.19840604873241227)
(490, 0.19840604873241227)
(491, 0.19840605464139477)
(492, 0.1984060596603763)
(493, 0.1984060596603758)
(494, 0.19840604872825504)
(495, 0.19840604872825482)
(496, 0.198406048723577)
(497, 0.19840606173818257)
(498, 0.19840604881677942)
(499, 0.19840604881677942)
(500, 0.19840604907409495)
(501, 0.19840604907409923)
(502, 0.19840604907409917)
(503, 0.1984066709201055)
(504, 0.1984066715454288)
(505, 0.19840667151905114)
(506, 0.19840667151170174)
(507, 0.1984066715116663)
(508, 0.19840667142446453)
(509, 0.19840667170607645)
(510, 0.19840667232588952)
(511, 0.19840667294256883)
(512, 0.19840667292384495)
(513, 0.19840667292380096)
(514, 0.19840667292380026)
(515, 0.19840667292375405)
(516, 0.19840667008955531)
(517, 0.19840667069785806)
(518, 0.1984066706966946)
(519, 0.19840667069337303)
(520, 0.1984066706933698)
(521, 0.19840667069335693)
(522, 0.198406658258861)
(523, 0.19840665809281782)
(524, 0.1984060669528969)
(525, 0.19840606717460346)
(526, 0.19840606717460332)
(527, 0.19840605606363024)
(528, 0.19840605608164277)
(529, 0.19840605609886394)
(530, 0.19840605609886394)
(531, 0.19840605609886394)
(532, 0.198406056116804)
(533, 0.1984060561207668)
(534, 0.19840605612078058)
(535, 0.19840605612078058)
(536, 0.19840605612078058)
(537, 0.19840605612078058)
(538, 0.19840605506553965)
(539, 0.19840605506553965)
(540, 0.19840605506553965)
(541, 0.19840605506553965)
(542, 0.1984060550691578)
(543, 0.1984060550691578)
(544, 0.19840608413353267)
(545, 0.19840608413353267)
(546, 0.19840608413353267)
(547, 0.1984060841177065)
(548, 0.19840608409157276)
(549, 0.19840608409157276)
(550, 0.19840603798604828)
(551, 0.19840603798604828)
(552, 0.19840603798604828)
(553, 0.19840603798604828)
(554, 0.198406037986141)
(555, 0.198406037986141)
(556, 0.19840608929500267)
(557, 0.19840608929500203)
(558, 0.19840608933203976)
(559, 0.19840608989248334)
(560, 0.19840609249226734)
(561, 0.19840609254194522)
(562, 0.19840609243986065)
(563, 0.19840609248903063)
(564, 0.19840609253797464)
(565, 0.19840725847954965)
(566, 0.1984072591894393)
(567, 0.19840725918931265)
(568, 0.19840725918931265)
(569, 0.19840725918931265)
(570, 0.19840726025589225)
(571, 0.19840736924433558)
(572, 0.19840616763626004)
(573, 0.19840617103700287)
(574, 0.19840617103700348)
(575, 0.198406159817318)
(576, 0.198406159817318)
(577, 0.19840668745139498)
(578, 0.19840668745138887)
(579, 0.19840668745138887)
(580, 0.19840668745138593)
(581, 0.19840668745138593)
(582, 0.1984066874513744)
(583, 0.19840668740282566)
(584, 0.19841316624989994)
(585, 0.19841220530499112)
(586, 0.19841221055542677)
(587, 0.19880199549749072)
(588, 0.19880201092863023)
(589, 0.198802343892093)
(590, 0.19880234998623786)
(591, 0.198890400948258)
(592, 0.19889066906535174)
(593, 0.19889066906012742)
(594, 0.19880386260930458)
(595, 0.1988041928878986)
(596, 0.1988041928878948)
(597, 0.19880452172286275)
(598, 0.1988048491308846)
(599, 0.19880506654773905)
(600, 0.19880539160161803)
(601, 0.19841353477464205)
(602, 0.19841353869533668)
(603, 0.19840796408406694)
(604, 0.1984079640840535)
(605, 0.1984079115901667)
(606, 0.1984079130744892)
(607, 0.20568764424381383)
(608, 0.2056555052951864)
(609, 0.20553049883410698)
(610, 0.20553049694361086)
(611, 0.20502824855510157)
(612, 0.20497561912736764)
(613, 0.20559447746688359)
(614, 0.20559447746688359)
(615, 0.20560002450770778)
(616, 0.2056000245077058)
(617, 0.20560002423887527)
(618, 0.20560554855537722)
(619, 0.2056091463804612)
(620, 0.20560914617661463)
(621, 0.20561463298148763)
(622, 0.2056164003797643)
(623, 0.20561640037569134)
(624, 0.2056163347702364)
(625, 0.20535756976353406)
(626, 0.2053039050396529)
(627, 0.20530390426857875)
(628, 0.19849525748804064)
(629, 0.19849532404200176)
(630, 0.1984953742345704)
(631, 0.19848451304570736)
(632, 0.19848451983529194)
(633, 0.19848401026683826)
(634, 0.19840747087635255)
(635, 0.19840747191900351)
(636, 0.19840747191900351)
(637, 0.19840747191900351)
(638, 0.19840747191900351)
(639, 0.1984074706014304)
(640, 0.1984074706014302)
(641, 0.19840747163943584)
(642, 0.19840747267339906)
(643, 0.19840747267205844)
(644, 0.19840747249242807)
(645, 0.198407472492492)
(646, 0.19840747317608218)
(647, 0.19840747317608218)
(648, 0.19840606240324463)
(649, 0.19840606240324463)
(650, 0.19840606240324463)
(651, 0.19840606240324463)
(652, 0.19840606240324463)
(653, 0.19840606242106454)
(654, 0.1984060624216331)
(655, 0.24464402436102334)
(656, 0.24464324692225708)
(657, 0.24466417796812717)
(658, 0.244695581656823)
(659, 0.49673173628782136)
(660, 0.4953890858645725)
(661, 0.49537803763854094)
(662, 0.4953928536451869)
(663, 0.49540266781538234)
(664, 0.49539040173122906)
(665, 0.19840605951435641)
(666, 0.19840605951435655)
(667, 0.19840605951435655)
(668, 0.19840605952930684)
(669, 0.19840607058505055)
(670, 0.19840607058505055)
(671, 0.19840607058958723)
(672, 0.19840604630992226)
(673, 0.19840604630992226)
(674, 0.19840604631592818)
(675, 0.19840603738571247)
(676, 0.19840603731178044)
(677, 0.19840603731178044)
(678, 0.19840603731178044)
(679, 0.19840603731178044)
(680, 0.19840603731176462)
(681, 0.19840603731176462)
(682, 0.19840603731176462)
(683, 0.19840603731256778)
(684, 0.19840603731257309)
(685, 0.19840603731257309)
(686, 0.19840603731257309)
(687, 0.19840603731257309)
(688, 0.19840603731257309)
(689, 0.19840603731876574)
(690, 0.19840603731876147)
(691, 0.19840603731876147)
(692, 0.19840603731996068)
(693, 0.1984060373199668)
(694, 0.19840603732016077)
(695, 0.19840603732016077)
(696, 0.1984060373201669)
(697, 0.1984060373201669)
(698, 0.1984060373201669)
(699, 0.19840603732017315)
(700, 0.19840603732017315)
(701, 0.19840603732017315)
(702, 0.19840603732017315)
(703, 0.19840603732037587)
(704, 0.1984060373217072)
(705, 0.1984060373217072)
(706, 0.19840603732171333)
(707, 0.19840603732171333)
(708, 0.19840603732178677)
(709, 0.19840603732179615)
(710, 0.19840603731801862)
(711, 0.19840603731801862)
(712, 0.19840603731801862)
(713, 0.1984060373180224)
(714, 0.1984060373180393)
(715, 0.19840603731804315)
(716, 0.19840603731809148)
(717, 0.19840603731809148)
(718, 0.19840603731795015)
(719, 0.19840603731795015)
(720, 0.19840603731795015)
(721, 0.19840603731795015)
(722, 0.19840603731192435)
(723, 0.19840603731192435)
(724, 0.19840603731192435)
(725, 0.19840603731166462)
(726, 0.19840603731166462)
(727, 0.19840603731166462)
(728, 0.19840603731166462)
(729, 0.19840603731166664)
(730, 0.19840603731166664)
(731, 0.19840603731166664)
(732, 0.19840603731166664)
(733, 0.19840603731159145)
(734, 0.19840603731159145)
(735, 0.19840603731159145)
(736, 0.19840603731159145)
(737, 0.19840603731159145)
(738, 0.19840603731159145)
(739, 0.19840603731159145)
(740, 0.19840603731159145)
(741, 0.19840603731159448)
(742, 0.19840603731159448)
(743, 0.19840603731159448)
(744, 0.19840603731159448)
(745, 0.19840603731159448)
(746, 0.19840603731159448)
(747, 0.19840603731159448)
(748, 0.19840603731159448)
(749, 0.19840603731159448)
(750, 0.19840603731159726)
(751, 0.19840603731159726)
(752, 0.19840603731159726)
(753, 0.19840603731159726)
(754, 0.19840603731159726)
(755, 0.1984060373115967)
(756, 0.1984060373115967)
(757, 0.1984060373115967)
(758, 0.1984060373115967)
(759, 0.1984060373115967)
(760, 0.19840603731159656)
(761, 0.19840603731156278)
(762, 0.19840603731156278)
(763, 0.19840603731156278)
(764, 0.19840603731156278)
(765, 0.19840603731156425)
(766, 0.19840603731156425)
(767, 0.19840603731156425)
(768, 0.19840603731156425)
(769, 0.19840603731156425)
(770, 0.19840603731156425)
(771, 0.1984060373115676)
(772, 0.1984060373115676)
(773, 0.19840603731425582)
(774, 0.19840603731425582)
(775, 0.19840603731425582)
(776, 0.19840603730712036)
(777, 0.19840603730712036)
(778, 0.19840603730712036)
(779, 0.19840603730712036)
(780, 0.19840603730712036)
(781, 0.19840603730712036)
(782, 0.19840603730712036)
(783, 0.19840603730712036)
(784, 0.19840603730712036)
(785, 0.19840603730712036)
(786, 0.19840603730712036)
(787, 0.19840603730712036)
(788, 0.19840603730712036)
(789, 0.19840603730712036)
(790, 0.19840603730712036)
(791, 0.198406037307251)
(792, 0.198406037307251)
(793, 0.198406037307251)
(794, 0.198406037307251)
(795, 0.19840603730724624)
(796, 0.19840603730724624)
(797, 0.19840603730724624)
(798, 0.19840603730724624)
(799, 0.19840603730724624)
(800, 0.19840603730724624)
(801, 0.1984060373072464)
(802, 0.1984060373072464)
(803, 0.1984060373072464)
(804, 0.1984060373072464)
(805, 0.1984060373072464)
(806, 0.1984060373072464)
(807, 0.1984060373072464)
(808, 0.19840603730723905)
(809, 0.19840603730723905)
(810, 0.19840603730723905)
(811, 0.19840603730723905)
(812, 0.19840603730723905)
(813, 0.19840603730723905)
(814, 0.19840603730723905)
(815, 0.19840603730723905)
(816, 0.19840603730723905)
(817, 0.19840603730723905)
(818, 0.19840603730723905)
(819, 0.19840603730723905)
(820, 0.19840603730723905)
(821, 0.19840603730723905)
(822, 0.19840603730723905)
(823, 0.19840603730723905)
(824, 0.19840603730723905)
(825, 0.19840603730723905)
(826, 0.19840603730723905)
(827, 0.19840603730723905)
(828, 0.19840603730723905)
(829, 0.19840603730723905)
(830, 0.19840603730723905)
(831, 0.19840603730723905)
(832, 0.19840603730723905)
(833, 0.19840603730723905)
(834, 0.19840603730742137)
(835, 0.19840603730742137)
(836, 0.19840603730536593)
(837, 0.19840603730536593)
(838, 0.19840603730536593)
(839, 0.19840603730536593)
(840, 0.19840603730536593)
(841, 0.19840603730536593)
(842, 0.19840603730536593)
(843, 0.19840603730536593)
(844, 0.19840603730536593)
(845, 0.19840603730536593)
(846, 0.19840603730536593)
(847, 0.19840603730536593)
(848, 0.19840603730536593)
(849, 0.19840603730536593)
(850, 0.19840603730536593)
(851, 0.19840603730536593)
(852, 0.19840603730536593)
(853, 0.19840603730536593)
(854, 0.19840603730536593)
(855, 0.19840603730536593)
(856, 0.19840603730536593)
(857, 0.19840603730536635)
(858, 0.19840603730537693)
(859, 0.19840603730537693)
(860, 0.19840603730537693)
(861, 0.19840603730537693)
(862, 0.19840603730538645)
(863, 0.19840603730538645)
(864, 0.19840603730538645)
(865, 0.19840603730538645)
(866, 0.19840603730538645)
(867, 0.19840603730538645)
(868, 0.19840603730538645)
(869, 0.19840603730538645)
(870, 0.19840603730538645)
(871, 0.19840603730538645)
(872, 0.19840603730538645)
(873, 0.19840603730538645)
(874, 0.19840603730538645)
(875, 0.19840603730538645)
(876, 0.19840603730538645)
(877, 0.19840603730538645)
(878, 0.19840603730538645)
(879, 0.19840603730538645)
(880, 0.19840603730538645)
(881, 0.19840603730538645)
(882, 0.19840603730538645)
(883, 0.19840603730538645)
(884, 0.19840603730535714)
(885, 0.19840603730535714)
(886, 0.19840603730535714)
(887, 0.19840603730535714)
(888, 0.19840603730535727)
(889, 0.19840603730535727)
(890, 0.19840603730535727)
(891, 0.19840603730535727)
(892, 0.1984060375513082)
(893, 0.1984060375513082)
(894, 0.19840603755131275)
(895, 0.19840603755131345)
(896, 0.19840603755131345)
(897, 0.19840603755233838)
(898, 0.19840603755233838)
(899, 0.19840603755233838)
(900, 0.19840603755233838)
(901, 0.19840603755197023)
(902, 0.19840603755197023)
(903, 0.19840603755197023)
(904, 0.19840603755184189)
(905, 0.1984060375518414)
(906, 0.1984060375518414)
(907, 0.1984060375518414)
(908, 0.1984060375518414)
(909, 0.1984060375518289)
(910, 0.1984060375518289)
(911, 0.1984060375518289)
(912, 0.19840603755182862)
(913, 0.1984060375518505)
(914, 0.1984060375518505)
(915, 0.1984060375518505)
(916, 0.1984060375518505)
(917, 0.1984060375518505)
(918, 0.1984060375518505)
(919, 0.1984060375518505)
(920, 0.1984060375518505)
(921, 0.1984060373081124)
(922, 0.1984060373081124)
(923, 0.1984060373081124)
(924, 0.19840603730811185)
(925, 0.19840603730811185)
(926, 0.19840603730811185)
(927, 0.19840603730811185)
(928, 0.19840603730811185)
(929, 0.19840603730811185)
(930, 0.19840603730811185)
(931, 0.19840603730811185)
(932, 0.19840603730811185)
(933, 0.19840603730811185)
(934, 0.19840603730811157)
(935, 0.19840603730811157)
(936, 0.19840603730811254)
(937, 0.19840603730811268)
(938, 0.19840603730811268)
(939, 0.19840603730811268)
(940, 0.19840603730811374)
(941, 0.19840603730811374)
(942, 0.19840603730811374)
(943, 0.19840603730811374)
(944, 0.19840603730811374)
(945, 0.19840603730811374)
(946, 0.19840603730811374)
(947, 0.19840603730811374)
(948, 0.19840603730811296)
(949, 0.19840603730811296)
(950, 0.1984060373081134)
(951, 0.1984060373081134)
(952, 0.19840603730811368)
(953, 0.19840603731081552)
(954, 0.19840603731081552)
(955, 0.19840603731081552)
(956, 0.19840603731081552)
(957, 0.19840603731081552)
(958, 0.19840603731081552)
(959, 0.19840603731081552)
(960, 0.19840603731081552)
(961, 0.19840603731081552)
(962, 0.19840603731081552)
(963, 0.19840603731081552)
(964, 0.19840603731081552)
(965, 0.19840603731081544)
(966, 0.19840603731081544)
(967, 0.19840603731081544)
(968, 0.19840603731098974)
(969, 0.19840603731098974)
(970, 0.19840603731098994)
(971, 0.19840603731099007)
(972, 0.19840603731099168)
(973, 0.19840603731099182)
(974, 0.19840603731096673)
(975, 0.19840603731096673)
(976, 0.19840603738242757)
(977, 0.19840603738242785)
(978, 0.1984195093533104)
(979, 0.1984195093531834)
(980, 0.19841950935311486)
(981, 0.22472977713785502)
(982, 0.22472924500013547)
(983, 0.22474083809821685)
(984, 0.2247410805749713)
(985, 0.22474108153134773)
(986, 0.22474136161283134)
(987, 0.22475292474801242)
(988, 0.22476445843297202)
(989, 0.22477493156682035)
(990, 0.22478626066747967)
(991, 0.22479389356340915)
(992, 0.22480531410063512)
(993, 0.22480531410330074)
(994, 0.22481671220697055)
(995, 0.21740701176658067)
(996, 0.21740701176896646)
(997, 0.21741518377635305)
(998, 0.21742058818140014)
(999, 0.21742056515476432)

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
