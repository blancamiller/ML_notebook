{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW9: Hard-Margin Classification with Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blanca Miller\n",
    "STAT 760\n",
    "04/19/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objective:__ Generate a training set for two classes that are linearly separable add noise to the set, then find a Support Vector Machine (SVM) that separates the two classes. \n",
    "\n",
    "\n",
    "__Support Vector Machines (SVM):__ SVMs find the maximum-margin hyperplane that divides the group of points xi for which yi=1 and yi=-1. The hyperplane is optimized by distance, that is, the distance between the hyperplane and the nearest points for the two classes, is maximized. \n",
    "- Hyperplane function: w * x - b = 0 \n",
    "- Hyperplane offset function from the origin along the normal vector w: b / ||w|| \n",
    "\n",
    "Because our training data is linearly separable, we can use two parallel hyperplanes to optimally separate the two classes, y=1 and y=-1. Optimal in the sense that \"good\" separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of the different classes. The two parallel hyperplanes are bounded by a margin. In general, the larger the margin between the two parallel hyperplanes, the lower the generalization error of the classifier. The maximum-margin hyperplane is now the hyperplane that lies between the two parallel hyperplanes:\n",
    "\n",
    "    ---------------------------------- parallel hyperplane 1\n",
    "    ================================== maximum-margin hyperplane\n",
    "    ---------------------------------- parallel hyperplane 2\n",
    "    \n",
    "Now we can describe the classes in terms of their relationship with the hyperplane:\n",
    "- Class y=1: w * x - b = 1 , where all data on or above this boundary is of class y=1\n",
    "- Class y=-1: w * x -b = -1  , where all data on or below this boundary is of class y=-1 \n",
    "\n",
    "The margin, or distance, between the two hyperplanes is: 2 / ||w|| To mazimize the margin we minimize the normal vector ||w||. Then, compute the distance from each edge point, the support vector, to a plane using the distance from a point to a plane equation. However, we first have to prevent data from falling into the margin, so we set the following constraints so that each data point is controlled to lie on the correct side of the margin: \n",
    "- w * xi - b >= 1 , if y=1\n",
    "- w * xi - b <= -1 , if y=-1 \n",
    "\n",
    "====> The two constraints can be collapsed into one function:  yi(w * xi - b) >= 1 , for all 1<=i<=n\n",
    "\n",
    "__Goal:__ \n",
    "Minimize the normal vector ||w|| subject to yi(w \\* xi - b) >= for all i=1,...,n where w & b determine the classifier: x |--> sgn(w \\* x - b)\n",
    "\n",
    "SVM Algorithm\n",
    "1. Given a data set: (x1, y1),...,(xn, yn) where \n",
    "- yi is either 1 or -1\n",
    "- xi is a p-dimensional real vector\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
